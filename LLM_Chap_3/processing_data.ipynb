{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40fa0e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 15:38:40.829338: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-17 15:38:48.423884: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-17 15:38:51.341421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int32 and shape [2]\n",
      "\t [[{{node Placeholder/_3}}]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1610, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m model.compile(optimizer=\u001b[33m\"\u001b[39m\u001b[33madam\u001b[39m\u001b[33m\"\u001b[39m, loss=\u001b[33m\"\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m labels = tf.convert_to_tensor([\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/conda_env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1214\u001b[39m, in \u001b[36mTFPreTrainedModel.train_on_batch\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1211\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(keras.Model.train_on_batch)\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_on_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1213\u001b[39m     args, kwargs = convert_batch_encoding(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py:2510\u001b[39m, in \u001b[36mModel.train_on_batch\u001b[39m\u001b[34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[39m\n\u001b[32m   2506\u001b[39m     iterator = data_adapter.single_batch_iterator(\n\u001b[32m   2507\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute_strategy, x, y, sample_weight, class_weight\n\u001b[32m   2508\u001b[39m     )\n\u001b[32m   2509\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_function = \u001b[38;5;28mself\u001b[39m.make_train_function()\n\u001b[32m-> \u001b[39m\u001b[32m2510\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2512\u001b[39m logs = tf_utils.sync_to_numpy_or_python_type(logs)\n\u001b[32m   2513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/conda_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filedjkeo3v_.py:15\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(\u001b[38;5;28mself\u001b[39m), ag__.ld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     17\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py:1268\u001b[39m, in \u001b[36mModel.make_train_function.<locals>.step_function\u001b[39m\u001b[34m(model, iterator)\u001b[39m\n\u001b[32m   1264\u001b[39m     run_step = tf.function(\n\u001b[32m   1265\u001b[39m         run_step, jit_compile=\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1266\u001b[39m     )\n\u001b[32m   1267\u001b[39m data = \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1269\u001b[39m outputs = reduce_per_replica(\n\u001b[32m   1270\u001b[39m     outputs,\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m   1272\u001b[39m     reduction=\u001b[38;5;28mself\u001b[39m.distribute_reduction_method,\n\u001b[32m   1273\u001b[39m )\n\u001b[32m   1274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py:1249\u001b[39m, in \u001b[36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_step\u001b[39m(data):\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tf.control_dependencies(_minimum_control_deps(outputs)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/conda_env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1610\u001b[39m, in \u001b[36mTFPreTrainedModel.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._using_dummy_loss \u001b[38;5;129;01mand\u001b[39;00m parse(tf.__version__) < parse(\u001b[33m\"\u001b[39m\u001b[33m2.11.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1608\u001b[39m     \u001b[38;5;66;03m# Newer TF train steps leave this out\u001b[39;00m\n\u001b[32m   1609\u001b[39m     data = expand_1d(data)\n\u001b[32m-> \u001b[39m\u001b[32m1610\u001b[39m x, y, sample_weight = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43munpack_x_y_sample_weight\u001b[49m(data)\n\u001b[32m   1611\u001b[39m \u001b[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001b[39;00m\n\u001b[32m   1612\u001b[39m \u001b[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001b[39;00m\n\u001b[32m   1613\u001b[39m \u001b[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001b[39;00m\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[31mAttributeError\u001b[39m: in user code:\n\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/brook/anaconda3/envs/conda_env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1610, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n",
    "\n",
    "# This is new\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "labels = tf.convert_to_tensor([1, 1])\n",
    "model.train_on_batch(batch, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
